# ğŸ§ Multimodal Speech Emotion Recognition (Audio + Text)

This project presents a **Multimodal Speech Emotion Recognition (SER)** system that combines **audio acoustic features** with **textual emotion descriptions** to improve emotion classification performance.  
By fusing complementary information from both modalities, the proposed approach significantly outperforms unimodal baselines.

---

## ğŸ“Œ Project Motivation

Traditional speech emotion recognition systems rely only on acoustic features, which may fail to capture semantic or contextual cues.  
To overcome this limitation, this project integrates:

- **Audio-based emotional characteristics**
- **Text-based semantic emotion descriptions**

The fusion enables more robust and interpretable emotion recognition.

---

## ğŸ¯ Objectives

- Extract meaningful **audio features** from speech signals  
- Perform **audio data augmentation** to improve generalization  
- Generate **emotion-aware textual descriptions**  
- Convert text into numerical embeddings using **TFâ€“IDF**  
- Fuse audio and text representations  
- Train a **multimodal machine learning model**  
- Compare performance with unimodal baselines  

---

## ğŸ—‚ï¸ Dataset

- Speech emotion dataset with labels:
  - ANG â€” Angry  
  - HAP â€” Happy  
  - SAD â€” Sad  
  - FEA â€” Fear  
  - NEU â€” Neutral  

- Audio format: `.wav`
- Labels extracted directly from filenames

---

## âš™ï¸ System Architecture

The pipeline processes raw audio files through a multi-stage workflow involving feature extraction, data augmentation, and NLP-based feature fusion for final classification.

- **Audio (.wav)**
    - **Feature Extraction**
        - `RMS` (Root Mean Square Energy)
        - `ZCR` (Zero Crossing Rate)
        - `Spectral Centroid`
        - `Spectral Bandwidth`
        - `Spectral Rolloff`
        - `MFCC` (Mel-frequency Cepstral Coefficients)
    - **Audio Augmentation**
        - Pitch Shift
        - Time Stretch
        - Noise Injection
    - **Text Description Generation**
        - Emotion-aware sentences
    - **Text Embedding**
        - TFâ€“IDF Vectorization
    - **Feature Fusion**
        - Audio + Text Concatenation
    - **Classification**
        - Random Forest Classifier


---

## ğŸ”Š Audio Feature Extraction

Each speech sample is represented using:

| Feature | Description |
|------|------|
| RMS | Signal energy / loudness |
| ZCR | Voice excitation and sharpness |
| Spectral Centroid | Brightness of sound |
| Spectral Bandwidth | Frequency spread |
| Spectral Rolloff | High-frequency dominance |
| MFCC | Perceptual timbre features |

---

## ğŸ” Data Augmentation Techniques

To increase data diversity and reduce overfitting:

- **Pitch Shifting** â€“ simulates different vocal tones  
- **Time Stretching** â€“ simulates speaking speed variations  
- **Noise Injection** â€“ improves robustness to real-world noise  

Augmentation increased dataset size to nearly **30,000 samples**.

---

## ğŸ“ Text Description Generation

For each audio sample, a corresponding text description is generated using:

- Neutral base sentences  
- Emotion-specific modifiers  
- Controlled randomness to avoid label leakage  

Example:â€œThe speaker says a short sentence with a slightly strong tense tone.â€
This simulates semantic emotion cues without using real transcripts.

---

## ğŸ”¤ Text Embedding

Text descriptions are converted into numerical representations using:

- **TFâ€“IDF Vectorizer**
- Limited feature dimensions for fast computation
- Efficient CPU-based embedding generation

---

## ğŸ”— Multimodal Feature Fusion

Final feature vector is constructed as:X = [Audio Features | Text Embeddings]
- Early fusion strategy
- StandardScaler applied before training

---

## ğŸ¤– Model Training

- Classifier: **Random Forest**
- Balanced class weights
- Stratified trainâ€“test split
- Hyperparameters tuned for stability and accuracy

---

## ğŸ“Š Results

| Model | Accuracy |
|------|------|
| Audio Only | ~62% |
| Text Only | ~39% |
| **Multimodal (Audio + Text)** | **~83%** |

### âœ… Key Observation

The multimodal model significantly outperforms unimodal approaches, confirming that **audio and text provide complementary emotional cues**.

---

## ğŸ“ˆ Visualizations Included

- Waveform & spectrogram comparison (original vs augmented)
- Audio feature distributions
- Pairplots across emotions
- Correlation heatmap
- Feature importance ranking
- RMS distribution after augmentation
- Final accuracy comparison plot

---

## ğŸš€ Technologies Used

- Python
- Librosa
- NumPy
- Pandas
- Scikit-learn
- Matplotlib
- Seaborn
- Jupyter Notebook

---

## ğŸ”® Future Work

- Use pretrained audio models (Wav2Vec2, HuBERT)
- Replace TFâ€“IDF with transformer-based text embeddings
- Integrate real speech transcripts
- Extend system for real-time emotion recognition
- Deploy as a web-based emotion analysis tool

---

## ğŸ‘¤ Authors

- Abhishek P
- Harsha
- Lohit J
  
CSE (AIML)  
PES University  

---



This project was developed as part of academic course work of Advanced Foundations for ML-UE23AM342AA1

